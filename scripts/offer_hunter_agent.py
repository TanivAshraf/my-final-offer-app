# -*- coding: utf-8 -*-
"""Offer Hunter Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HEgkRzJUMSIcEY1XhbgWlP_XZB3rEpdf
"""

!pip install requests beautifulsoup4

# Import the tools we need from our toolbox
import requests
from bs4 import BeautifulSoup
import time # A tool to help us pause

# --- This is our main "Hunter" tool ---
def scrape_url(url):
    print(f"--- Visiting: {url} ---")
    try:
        # We pretend to be a real browser so the website lets us in
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        # The 'requests' tool goes to the URL and brings back the webpage
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() # This checks if the website sent back an error

        # The 'BeautifulSoup' tool cleans up the messy HTML code
        soup = BeautifulSoup(response.content, 'html.parser')
        # We pull out all the clean text from the page
        page_text = soup.get_text(separator='\n', strip=True)
        return page_text

    except requests.exceptions.RequestException as e:
        print(f"!!! ERROR: Couldn't visit this page. Reason: {e} !!!")
        return None # If it fails, it returns nothing

# --- This is our list of targets ---
target_urls = [
"    https://www.bracbank.com/en/retail/card/extra/dining",
    "https://www.bracbank.com/en/retail/card",
    "https://av.sc.com/bd/edm/b1g1-offers/",
    "https://www.sc.com/bd/promotions/",
    "https://www.sc.com/bd/credit-cards/",
    "https://www.sc.com/bd/credit-cards/visa-smart-platinum/",
    "https://www.sc.com/bd/credit-cards/visa-signature/",
    "https://www.sc.com/bd/credit-cards/super-value-titanium/",
    "https://www.sc.com/bd/credit-cards/silver-visa-mastercard/",
    "https://www.sc.com/bd/credit-cards/assurance-credit-card/",
    "https://www.sc.com/in/credit-cards/offers/",
    "https://www.bracbank.com/en/retail/card/extra/freeoffer",
    "https://www.bracbank.com/en/retail/card/extra/lifestyle",
    "https://www.bracbank.com/en/retail/card/extra/hotel",
    "https://www.ebl.com.bd/retail/EBL-Cards",
    "https://www.ebl.com.bd/retail/eblcard/EBL-Visa-Classic-Credit-Card",
    "https://www.ebl.com.bd/retail/eblcard/EBL-VISA-Gold-Credit-Card",
    "https://www.ebl.com.bd/retail/eblcard/EBL-VISA-Platinum-Credit-Card",
    "https://www.citybankplc.com/card/amex-cards",
    "https://www.citybankplc.com/platinum-reserve-credit-card/",
    "https://www.citybankplc.com/card/amex-platinum",
    "https://www.ucb.com.bd/cards/card-privileges",
    "https://www.americanexpress.com/en-bd/network/credit-cards/city-bank/gold-credit-card.html/",
    "https://www.americanexpress.com/en-bd/network/credit-cards/city-bank/platinum-credit-card.html/",
    "https://www.americanexpress.com/en-bd/network/credit-cards/city-bank/platinum-reserve-credit-card/"
]
# Note: I've shortened the list to 5 for our first test to make it faster.

# --- This is the main part of our agent's job ---
all_scraped_data = {} # An empty box to store the text we find

# We tell our agent to loop through every URL in our target list
for url in target_urls:
    # Use our hunter tool on the URL
    text_data = scrape_url(url)

    # If the tool found text, we save it
    if text_data:
        all_scraped_data[url] = text_data
        print(f"*** Success! Found lots of text. ***\n")
    else:
        print(f"*** Failed. Moving to the next target. ***\n")

    # We tell our agent to politely wait for 2 seconds before visiting the next site
    time.sleep(2)

print("--- HUNTING MISSION COMPLETE ---")
print(f"Successfully got text from {len(all_scraped_data)} out of {len(target_urls)} websites.")

!pip install langchain-google-genai

# --- All Our Tools from the Toolbox ---
import requests
from bs4 import BeautifulSoup
import time
from google.colab import userdata # The tool to get our secret key
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate

# --- Tool #1: The "Hunter" (Our Web Scraper) ---
# This is the same function as before. Its job is to get the raw text.
def scrape_url(url):
    print(f"--- Visiting: {url} ---")
    try:
        headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36' }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        return soup.get_text(separator='\n', strip=True)
    except requests.exceptions.RequestException as e:
        print(f"!!! ERROR: Couldn't visit this page. Reason: {e} !!!")
        return None

# --- Tool #2: The "Analyst" (Our AI Brain) ---
# This is our new tool. Its job is to read text and find offers.
def find_offers_in_text(text_data, api_key):
    print("--- Giving text to the AI Analyst to find offers... ---")
    try:
        # The prompt is our specific command to the AI
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", "You are an expert financial analyst. Your job is to read the messy text from a bank's webpage and identify all credit card offers. Respond with a simple bulleted list. For each offer, describe the offer and the merchant. If you find no offers, you MUST respond with the single phrase: 'No offers found.'"),
            ("human", "{text}")
        ])

        # We tell our AI which model to use and give it our secret key
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=api_key)

        # We create the "chain" that connects our prompt to the AI
        chain = prompt_template | llm

        # We send the text to the AI and get back its answer
        response = chain.invoke({"text": text_data})
        return response.content

    except Exception as e:
        print(f"!!! ERROR: The AI Analyst failed. Reason: {e} !!!")
        return "Error analyzing text."

# --- Getting our secret key securely ---
GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')

# --- Our list of targets (shortened for the test) ---
target_urls = [
    "https://www.sc.com/bd/promotions/",
    "https://www.ebl.com.bd/retail/EBL-Cards"
]

# --- The NEW Main Mission (Hunt and Analyze) ---
for url in target_urls:
    # Step 1: Hunt for the text using our first tool
    raw_text = scrape_url(url)

    if raw_text:
        # Step 2: If we found text, give it to our second tool to analyze
        offers_found = find_offers_in_text(raw_text, GEMINI_API_KEY)

        print("\n--- AI Analyst Report ---")
        print(offers_found)
        print("-------------------------\n")
    else:
        print("*** Failed to get text. Moving to next target. ***\n")

    time.sleep(2)

print("--- FULL MISSION COMPLETE ---")

!pip install supabase

!pip install langchain-text-splitters

!pip install selenium webdriver-manager

!apt-get update
!apt-get install -y chromium-chromedriver

# --- All Our Tools from the Toolbox ---
import time
import json
from google.colab import userdata

# The rest of our AI and Database tools
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from supabase import create_client, Client

# --- Getting our secret keys securely ---
GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')
SUPABASE_URL = userdata.get('SUPABASE_URL')
SUPABASE_KEY = userdata.get('SUPABASE_KEY')

# --- Tool #1: The "Analyst" (Our AI Brain) ---
def find_offers_in_text(text_data, api_key, source_url):
    print("--- Giving text to the AI Analyst to find offers... ---")
    try:
        prompt_text = """
        From the text below, extract all credit card offers into a JSON list.
        Each object in the list MUST have these keys: "bank_name", "card_name", "merchant_name", "offer_details".
        If a value is not found, use the string "Not specified".
        If there are absolutely no offers in the text, you MUST respond with an empty list: [].

        TEXT TO ANALYZE:
        {text}
        """
        prompt_template = ChatPromptTemplate.from_template(prompt_text)

        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=api_key)
        chain = prompt_template | llm

        response = chain.invoke({"text": text_data})
        ai_response_text = response.content

        start_index = ai_response_text.find('[')
        end_index = ai_response_text.rfind(']')

        if start_index != -1 and end_index != -1:
            json_str = ai_response_text[start_index:end_index+1]
            offers_in_chunk = json.loads(json_str)
            for offer in offers_in_chunk:
                offer['source_url'] = source_url
                offer['disclaimer'] = "This is unverified data scraped from the source URL. Use at your own risk."
            print(f"--- Found {len(offers_in_chunk)} potential offers in this text. ---")
            return offers_in_chunk
        else:
            print(f"!!! AI Analyst Warning: No valid JSON list found. Response was: '{ai_response_text}' !!!")
            return []

    except Exception as e:
        print(f"!!! ERROR analyzing text. Reason: {e} !!!")
        return []

# --- Tool #2: The "Reporter" (Our Database Saver) ---
def save_offers_to_db(supabase_client, offers_list):
    clean_offers = [offer for offer in offers_list if offer.get("offer_details") and "not specified" not in offer.get("offer_details").lower()]
    print(f"--- Reporting {len(clean_offers)} new, clean offers to the database... ---")
    try:
        if not clean_offers:
            print("--- No new clean offers to report. ---")
            return

        # --- THE FIX IS ON THIS LINE ---
        data, count = supabase_client.table('offer').insert(clean_offers).execute()

        print(f"*** VICTORY! Saved {len(data[1])} offers to the database! ***")
    except Exception as e:
        print(f"!!! ERROR: Could not save to database. Reason: {e} !!!")

# --- Setting up the connection ---
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
print("--- Successfully connected to the Supabase database! ---")

# --- Our High-Quality, Hand-Picked "Evidence" ---
source_website = "https://www.thecitybank.com/offers"
clean_text_with_offers = """
Enjoy a BOGO offer at The Westin Dhaka with your City Bank Amex Platinum card.
Also, get 20% off at Pizza Hut using any UCB Visa card.
Brac Bank offers a 15% discount at Le Meridien for all Signature cardholders.
"""

# --- The FINAL, FOCUSED MISSION ---
# 1. Analyze the clean text
found_offers = find_offers_in_text(clean_text_with_offers, GEMINI_API_KEY, source_website)

# 2. Save the results
save_offers_to_db(supabase, found_offers)

print("--- FULL AGENT MISSION COMPLETE ---")

# --- All Our Tools from the Toolbox ---
import time
import json
from google.colab import userdata

# --- Selenium browser tools ---
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service

# The rest of our AI and Database tools
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
from supabase import create_client, Client

# --- Getting our secret keys securely ---
GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')
SUPABASE_URL = userdata.get('SUPABASE_URL')
SUPABASE_KEY = userdata.get('SUPABASE_KEY')

# --- Tool #1: The "Hunter" (Selenium version) ---
def scrape_url(url):
    print(f"--- Visiting with a real browser: {url} ---")
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        service = Service()
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.get(url)
        time.sleep(5)
        html_content = driver.page_source
        driver.quit()
        soup = BeautifulSoup(html_content, 'html.parser')
        return soup.get_text(separator='\n', strip=True)
    except Exception as e:
        print(f"!!! ERROR: The browser failed. Reason: {e} !!!")
        if 'driver' in locals():
            driver.quit()
        return None

# --- Tool #2: The "Analyst" (Our AI Brain) ---
def find_offers_in_text(text_data, api_key, source_url):
    print("--- Giving text to the AI Analyst to find offers... ---")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=500)
    chunks = text_splitter.split_text(text_data)
    print(f"--- Broke the text into {len(chunks)} smaller chunks. Analyzing each one... ---")
    all_offers_found = []
    try:
        prompt_text = """
        From the text below, extract all credit card offers into a JSON list.
        Each object in the list MUST have these keys: "bank_name", "card_name", "merchant_name", "offer_details".
        If a value is not found, use the string "Not specified".
        If there are absolutely no offers in the text, you MUST respond with an empty list: [].

        TEXT TO ANALYZE:
        {text}
        """
        prompt_template = ChatPromptTemplate.from_template(prompt_text)
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=api_key)
        chain = prompt_template | llm
        for i, chunk in enumerate(chunks):
            print(f"--- Analyzing chunk {i+1}/{len(chunks)}... ---")
            try:
                response = chain.invoke({"text": chunk})
                ai_response_text = response.content
                start_index = ai_response_text.find('[')
                end_index = ai_response_text.rfind(']')
                if start_index != -1 and end_index != -1:
                    json_str = ai_response_text[start_index:end_index+1]
                    offers_in_chunk = json.loads(json_str)
                    for offer in offers_in_chunk:
                        offer['source_url'] = source_url
                        offer['disclaimer'] = "This is unverified data scraped from the source URL. Use at your own risk."
                    all_offers_found.extend(offers_in_chunk)
                    print(f"--- Found {len(offers_in_chunk)} potential offers in this chunk. ---")
                else:
                    print(f"!!! AI Analyst Warning: No valid JSON list found in this chunk. Skipping. !!!")
                time.sleep(5) # Be polite to the API
            except Exception as e:
                print(f"!!! ERROR analyzing chunk {i+1}. Reason: {e} !!!")
                continue
        return all_offers_found
    except Exception as e:
        print(f"!!! ERROR: The AI Analyst failed. Reason: {e} !!!")
        return []

# --- Tool #3: The "Reporter" (Our Database Saver) ---
def save_offers_to_db(supabase_client, offers_list):
    clean_offers = [offer for offer in offers_list if offer.get("offer_details") and "not specified" not in offer.get("offer_details").lower()]
    print(f"--- Reporting {len(clean_offers)} new, clean offers to the database... ---")
    try:
        if not clean_offers:
            print("--- No new clean offers to report from this URL. ---")
            return
        data, count = supabase_client.table('offer').insert(clean_offers).execute()
        print(f"*** Success! Saved {len(data[1])} offers to the database. ***")
    except Exception as e:
        print(f"!!! ERROR: Could not save to database. Reason: {e} !!!")

# --- Setting up the connection ---
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
print("--- Successfully connected to the Supabase database! ---")

# --- Our FULL list of production targets ---
target_urls = [
    "https://www.bracbank.com/en/retail/card/extra/dining",
    "https://www.bracbank.com/en/retail/card/extra/lifestyle",
    "https://www.bracbank.com/en/retail/card/extra/hotel",
    "https://av.sc.com/bd/edm/b1g1-offers/",
    "https://www.sc.com/bd/promotions/",
    "https://www.ebl.com.bd/retail/EBL-Cards",
    "https://www.citybankplc.com/card/amex-cards",
    "https://www.ucb.com.bd/cards/card-privileges"
]

# --- The FINAL Main Mission ---
for url in target_urls:
    raw_text = scrape_url(url)
    if raw_text:
        found_offers = find_offers_in_text(raw_text, GEMINI_API_KEY, url)
        save_offers_to_db(supabase, found_offers)
    else:
        print("*** Failed to get text. Moving on. ***")
    print("--------------------------------------------------\n")
    time.sleep(3)

print("--- FULL AGENT MISSION COMPLETE ---")